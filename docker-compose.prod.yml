# Production Docker Compose for AWS EC2
# Usage: docker compose -f docker-compose.prod.yml up -d

services:
  # ─── Reverse Proxy (auto-HTTPS via Let's Encrypt) ───
  caddy:
    image: caddy:2-alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./deploy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      - backend
    networks:
      - app

  # ─── Go Backend API ───
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    restart: unless-stopped
    expose:
      - "8080"
    env_file:
      - .env.production
    environment:
      - PORT=8080
      - DATABASE_URL=postgres://${POSTGRES_USER:-paper}:${POSTGRES_PASSWORD:-paper}@postgres:5432/${POSTGRES_DB:-paper}?sslmode=disable
      - OPENSEARCH_URL=http://opensearch:9200
      - OPENSEARCH_INDEX=papers
    depends_on:
      postgres:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    networks:
      - app

  # ─── PostgreSQL Database ───
  postgres:
    image: postgres:15-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-paper}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-paper}
      POSTGRES_DB: ${POSTGRES_DB:-paper}
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./backend/migrations/init.sql:/docker-entrypoint-initdb.d/001_init.sql:ro
      - ./backend/migrations/002_add_citation_count.sql:/docker-entrypoint-initdb.d/002_add_citation_count.sql:ro
      - ./backend/migrations/003_optimize_bulk_ingest.sql:/docker-entrypoint-initdb.d/003_optimize_bulk_ingest.sql:ro
      - ./backend/migrations/004_enrich_metadata.sql:/docker-entrypoint-initdb.d/004_enrich_metadata.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-paper} -d ${POSTGRES_DB:-paper}"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 15s
    # Postgres tuning for search-heavy workload
    command: >
      postgres
        -c shared_buffers=256MB
        -c effective_cache_size=768MB
        -c maintenance_work_mem=128MB
        -c work_mem=16MB
        -c max_connections=100
        -c wal_buffers=16MB
        -c random_page_cost=1.1
        -c effective_io_concurrency=200
        -c log_min_duration_statement=500
    networks:
      - app

  # ─── OpenSearch (single-node, scalable later) ───
  opensearch:
    image: opensearchproject/opensearch:2.11.1
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - DISABLE_INSTALL_DEMO_CONFIG=true
      - DISABLE_SECURITY_PLUGIN=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - osdata:/usr/share/opensearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - app

  # ─── OpenSearch Dashboards (optional, for debugging) ───
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.1
    restart: unless-stopped
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    ports:
      - "5601:5601"
    depends_on:
      opensearch:
        condition: service_healthy
    networks:
      - app
    profiles:
      - debug

volumes:
  pgdata:
    driver: local
  osdata:
    driver: local
  caddy_data:
    driver: local
  caddy_config:
    driver: local

networks:
  app:
    driver: bridge
